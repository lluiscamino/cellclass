{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Práctica 1: Clasificación de células\n",
    "\n",
    "## Enunciado del problema\n",
    "La anemia de células falciformes es una alteración de la sangre que hace que los glóbulos rojos\n",
    "se deformen hasta adquirir una forma elongada, en vez de circular.\n",
    "\n",
    "Esta práctica consiste en clasificar un conjunto de células en tres clases diferentes\n",
    "(circulares - c, elongadas - e u otras - o) usando el **mínimo de características posibles**.\n",
    "\n",
    "<img alt=\"Glóbulos rojos en la sangre\" src=\"example.jpeg\" width=\"600\"/>\n",
    "\n",
    "## Descripción del dataset\n",
    "El dataset está formado por cuatro archivos CSV que contienen diferentes atributos sobre cada célula.\n",
    "Cada célula está identificada por un ID único, para poder ser reconocida en los diferentes archivos.\n",
    "\n",
    "Los cuatro archivos de datos son los siguientes:\n",
    "- `info.csv`: Contiene el path a la imagen de la célula en cuestión y su clase.\n",
    "- `color.csv`: Contiene todas las características referentes al color extraídas de cada célula.\n",
    "- `shape.csv`: Contiene todas las características referentes a la forma extraídas de cada célula.\n",
    "- `texture.csv`: Contiene todas las características referentes a la textura extraídas de cada célula.\n",
    "\n",
    "Estas características han sido extraídas mediante técnicas de visión por computador.\n",
    "El dataset está formado completamente por datos numéricos y no contiene datos inválidos o vacíos.\n",
    "\n",
    "## Importación del dataset\n",
    "Para poder tratar este dataset de manera eficiente, se ha generado un nuevo fichero CSV que\n",
    "une los datos de los tres cuatro archivos de datos.\n",
    "\n",
    "Para agregar los datos en un solo fichero, se ha desarrollado un script, que es capaz de unir en un\n",
    "archivo los datos de entrenamiento así como los datos de test.\n",
    "\n",
    "## Análisis exploratorio de datos\n",
    "Una vez generado el fichero que une todos los datos de entrenamiento, se ha podido comprobar como este\n",
    "contiene 121 columnas (variables) y 445 filas (muestras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lluis/Desktop/cellclass/utils/utils.py:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop(\"idx\", 1)\n",
      "/Users/lluis/Desktop/cellclass/utils/utils.py:15: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = data.drop(\"path\", 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(445, 121)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils import get_data\n",
    "\n",
    "X, y = get_data()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A continuación podemos ver una muestra de las características del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue mean</th>\n",
       "      <th>Green mean</th>\n",
       "      <th>Red mean</th>\n",
       "      <th>Blue std</th>\n",
       "      <th>Green std</th>\n",
       "      <th>Red std</th>\n",
       "      <th>Hue mean</th>\n",
       "      <th>Saturation mean</th>\n",
       "      <th>Value mean</th>\n",
       "      <th>Hue std</th>\n",
       "      <th>...</th>\n",
       "      <th>Correlation3</th>\n",
       "      <th>Correlation4</th>\n",
       "      <th>Correlation5</th>\n",
       "      <th>Correlation6</th>\n",
       "      <th>Correlation7</th>\n",
       "      <th>Correlation8</th>\n",
       "      <th>Correlation9</th>\n",
       "      <th>Correlation10</th>\n",
       "      <th>Correlation11</th>\n",
       "      <th>Correlation12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150.2912</td>\n",
       "      <td>118.8615</td>\n",
       "      <td>130.3895</td>\n",
       "      <td>5.2289</td>\n",
       "      <td>8.8786</td>\n",
       "      <td>8.4380</td>\n",
       "      <td>131.4628</td>\n",
       "      <td>53.4965</td>\n",
       "      <td>150.4433</td>\n",
       "      <td>10.9909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8115</td>\n",
       "      <td>0.8603</td>\n",
       "      <td>0.8183</td>\n",
       "      <td>0.8570</td>\n",
       "      <td>0.8081</td>\n",
       "      <td>0.8603</td>\n",
       "      <td>0.7689</td>\n",
       "      <td>0.7501</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.7627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158.2896</td>\n",
       "      <td>126.3923</td>\n",
       "      <td>144.6604</td>\n",
       "      <td>6.0253</td>\n",
       "      <td>10.1304</td>\n",
       "      <td>9.7674</td>\n",
       "      <td>136.0408</td>\n",
       "      <td>53.0517</td>\n",
       "      <td>159.4682</td>\n",
       "      <td>18.7224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7850</td>\n",
       "      <td>0.8285</td>\n",
       "      <td>0.7694</td>\n",
       "      <td>0.8347</td>\n",
       "      <td>0.7821</td>\n",
       "      <td>0.8285</td>\n",
       "      <td>0.7014</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>0.7294</td>\n",
       "      <td>0.7250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>154.9417</td>\n",
       "      <td>127.0161</td>\n",
       "      <td>143.5692</td>\n",
       "      <td>5.2608</td>\n",
       "      <td>10.3249</td>\n",
       "      <td>9.3363</td>\n",
       "      <td>138.2455</td>\n",
       "      <td>46.8765</td>\n",
       "      <td>155.5516</td>\n",
       "      <td>15.9481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7870</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.8651</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>0.7378</td>\n",
       "      <td>0.7645</td>\n",
       "      <td>0.7440</td>\n",
       "      <td>0.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.9344</td>\n",
       "      <td>115.6617</td>\n",
       "      <td>132.1807</td>\n",
       "      <td>8.5218</td>\n",
       "      <td>13.4602</td>\n",
       "      <td>14.1391</td>\n",
       "      <td>132.4725</td>\n",
       "      <td>63.1637</td>\n",
       "      <td>153.4545</td>\n",
       "      <td>21.6648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7484</td>\n",
       "      <td>0.8133</td>\n",
       "      <td>0.7608</td>\n",
       "      <td>0.8399</td>\n",
       "      <td>0.7608</td>\n",
       "      <td>0.8133</td>\n",
       "      <td>0.6838</td>\n",
       "      <td>0.7264</td>\n",
       "      <td>0.6996</td>\n",
       "      <td>0.6775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152.5914</td>\n",
       "      <td>123.9839</td>\n",
       "      <td>138.9914</td>\n",
       "      <td>4.7207</td>\n",
       "      <td>8.9208</td>\n",
       "      <td>8.7410</td>\n",
       "      <td>136.2724</td>\n",
       "      <td>48.6253</td>\n",
       "      <td>153.1451</td>\n",
       "      <td>14.3680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8152</td>\n",
       "      <td>0.8573</td>\n",
       "      <td>0.8087</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>0.8146</td>\n",
       "      <td>0.8573</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>0.7627</td>\n",
       "      <td>0.7534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue mean  Green mean  Red mean  Blue std  Green std  Red std  Hue mean  \\\n",
       "0   150.2912    118.8615  130.3895    5.2289     8.8786   8.4380  131.4628   \n",
       "1   158.2896    126.3923  144.6604    6.0253    10.1304   9.7674  136.0408   \n",
       "2   154.9417    127.0161  143.5692    5.2608    10.3249   9.3363  138.2455   \n",
       "3   151.9344    115.6617  132.1807    8.5218    13.4602  14.1391  132.4725   \n",
       "4   152.5914    123.9839  138.9914    4.7207     8.9208   8.7410  136.2724   \n",
       "\n",
       "   Saturation mean  Value mean  Hue std  ...  Correlation3  Correlation4  \\\n",
       "0          53.4965    150.4433  10.9909  ...        0.8115        0.8603   \n",
       "1          53.0517    159.4682  18.7224  ...        0.7850        0.8285   \n",
       "2          46.8765    155.5516  15.9481  ...        0.7870        0.8440   \n",
       "3          63.1637    153.4545  21.6648  ...        0.7484        0.8133   \n",
       "4          48.6253    153.1451  14.3680  ...        0.8152        0.8573   \n",
       "\n",
       "   Correlation5  Correlation6  Correlation7  Correlation8  Correlation9  \\\n",
       "0        0.8183        0.8570        0.8081        0.8603        0.7689   \n",
       "1        0.7694        0.8347        0.7821        0.8285        0.7014   \n",
       "2        0.8015        0.8651        0.7957        0.8440        0.7378   \n",
       "3        0.7608        0.8399        0.7608        0.8133        0.6838   \n",
       "4        0.8087        0.8626        0.8146        0.8573        0.7469   \n",
       "\n",
       "   Correlation10  Correlation11  Correlation12  \n",
       "0         0.7501         0.7577         0.7627  \n",
       "1         0.7150         0.7294         0.7250  \n",
       "2         0.7645         0.7440         0.7277  \n",
       "3         0.7264         0.6996         0.6775  \n",
       "4         0.7651         0.7627         0.7534  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelos de aprendizaje automático\n",
    "Una vez explicado el enunciado del problema y visto cómo se han importado los datos, podemos pasar a\n",
    "explicar los diferentes pasos que se han seguido para obtener el modelo final.\n",
    "\n",
    "### División del conjunto de datos en train y test\n",
    "En primer lugar, se ha hecho una división de los datos de entrenamiento en dos subconjuntos, para poder\n",
    "entrenar y posteriormente, evaluar el rendimiento del modelo.\n",
    "\n",
    "Esta división se ha hecho con el método `train_test_split` de `scikit`, dándole al subconjunto de test\n",
    "un tamaño del 30% del conjunto global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Escalado de datos\n",
    "Adicionalmente, se han normalizado los datos mediante el escalado estándar usando la clase\n",
    "`StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import scale_data\n",
    "\n",
    "X_train, X_test = scale_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Perceptrón, regresión logística, Random Forest y SVM básicos\n",
    "La primera estrategia para intentar construir un modelo de aprendizaje automático capaz de clasificar\n",
    "correctamente las células, ha sido entrenar cuatro clasificadores distintos utilizando todas las\n",
    "características del dataset.\n",
    "\n",
    "#### Perceptrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 43  2]\n",
      " [ 1  2 36]]\n",
      "Precision:\t 0.9626865671641791\n",
      "Recall:\t 0.9626865671641791\n",
      "F1 score:\t 0.9626865671641791\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import print_model_performance_metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"perceptron\", eta0=1, max_iter=1000, learning_rate=\"constant\", random_state=5)\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "print_model_performance_metrics(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  0  2]\n",
      " [ 0 41  4]\n",
      " [ 2  2 35]]\n",
      "Precision:\t 0.9253731343283582\n",
      "Recall:\t 0.9253731343283582\n",
      "F1 score:\t 0.9253731343283582\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log\", eta0=1, max_iter=1000, learning_rate=\"constant\", random_state=5)\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "print_model_performance_metrics(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  0  0]\n",
      " [ 0 41  4]\n",
      " [ 0  1 38]]\n",
      "Precision:\t 0.9626865671641791\n",
      "Recall:\t 0.9626865671641791\n",
      "F1 score:\t 0.9626865671641791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "print_model_performance_metrics(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0  3]\n",
      " [ 0 42  3]\n",
      " [ 1  1 37]]\n",
      "Precision:\t 0.9402985074626866\n",
      "Recall:\t 0.9402985074626866\n",
      "F1 score:\t 0.9402985074626865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "svc.fit(X_train, y_train)\n",
    "prediction = svc.predict(X_test)\n",
    "\n",
    "print_model_performance_metrics(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparación\n",
    "| Clasificador       | F1 Score (test split)  | F1 Score (test data)  |\n",
    "| ------------------ |:----------------------:|:---------------------:|\n",
    "| Perceptrón         | 0.9626                 | 0.4808                |\n",
    "| Regresión logística| 0.9253                 | 0.3926                |\n",
    "| Random forest      | 0.9626                 | 0.2838                |\n",
    "| SVM                | 0.9402                 | 0.1367                |\n",
    "\n",
    "Podemos observar que, pese a que el rendimiento de todos los modelos parece ser bueno juzgando por el subconjunto de\n",
    "entrenamiento, una vez subido a la plataforma Kaggle para ser evaluado con el conjunto `test`, el rendimiento de ningún\n",
    "modelo es aceptable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}